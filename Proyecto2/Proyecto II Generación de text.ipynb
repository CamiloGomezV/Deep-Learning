{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto II: Generación automatica de texto\n",
    "\n",
    "En este poryecto el objetivo es desarrollar un modelo de generación aunotática de texto utilizando redes recurrentes.Para este modelo deben realizar los siguientes pasos:\n",
    "\n",
    "1. Importar un corpus (texto con el que van a entrenar el modelo). Este corpus debe ser grande y contener mas de 100 mil palabras. Para este proyecto se pedira que entrenen con dos corpus de estilo diferente. Por ejemplo, un corpus con las obras de gabriel garcia marquez, otro corpus con poemas.\n",
    "2. Preprocesar el texto. Tenga en cuenta que la entrada debe ser lo más homogenea posible, para poder entrenar las redes. Algunos modelos eliminan la puntuación, sin embargo, si esto se hace el texto generado no tendra puntuación. Yo recomiendo dejar estos elementos. Al igual que los saltos de lineas si entrenan poemas, o incluso en el texto para generar el punto aparte.\n",
    "3. Preparar los datos de entrenamiento, esto inclutye los wordembeddings, o la generación de indices que puedan ser utilziados en un acapa de embedding.\n",
    "4. Diseño de la arquitectura de red. Yo aconsejo utilizar un acapa de embeddings, para que el algoritmo por si solo aprenda el embedding apropiado para la tarea.\n",
    "5. generación de texto. Tenga en cuenta que al entrenar se entrena en modo profesor, pero al generar se debe retroalimentar la salida del modelo como entrada para producir las palabras siguientes. Para parar su ejecución pueden generar un token de finalizacion /<end/> y solo parar cuando este token aparezca, si este es el caso ese token debe estar en el diccionario y en el texto que esten procesando. Otra forma es generar x número de palabras y parar la ejecución una vez se haya cumplido con ese número de palabras.\n",
    "6. Para ejecutar el programa deben proporcionar una semilla, esta semilla debe contener palabras que esten en el vocabulario con el que se entreno, una vez hecho esto se puede correr el modelo.\n",
    "7. En la entrega deben especificar paso a paso el proceso realizado, comparar los resultados del texto generado con ambos modelos entrenados (uno para cada corpus) y discutir a profundidad los resultados obtenidos, problemas y posibles mejoras.\n",
    "\n",
    "Dado que este proyecto requiere el uso de temas no vistos a profundidad en clase, les proporciono el siguiente blog que pueden usar como base para el desarrollo del proyecto. Les recomiendo lo lean, lo sigan paso a paso, ejecuten el código que allí se encuentra y luego hagan las modificaciones necesarias para el desarrollo de su proyecto. El blog lo pueden encontrar en este [link](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/). \n",
    "\n",
    "El proyecto lo deben entregar el Lunes 24 de abril\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primer paso, se definen cuatro funciones. La primera función carga y lee los archivos de texto y retorna una única lista. La segunda función, se encarga de limpiar el texto, eliminando algunos caracteres especiales y castear (casting) las palabras a minúscula, esta función retorna una lista con todas las palabras de acuerdo con el orden que fueron escritas. La tercera función se encarga de guardar el texto ya procesado en un nuevo archivo de texto. Por último, la cuarta función recibe las palabras (tokens) y una longitud definida, dicha longitud define el tamaño de la secuencia que se generara a partir de las palabras de entrada. Estas cuatro funciones componen todo el preprocesamiento de texto para cada corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(filenames):\n",
    "    complete_text = []\n",
    "    for filename in filenames:\n",
    "        file = open(filename, 'r')\n",
    "        text = file.read()\n",
    "        complete_text += text\n",
    "        file.close()\n",
    "    return text\n",
    "\n",
    "def clean_text(doc):\n",
    "    doc = doc.replace('--', '')\n",
    "    doc = doc.replace('—', '')\n",
    "    doc = doc.replace('-', '')\n",
    "    doc = doc.replace('«', '')\n",
    "    doc = doc.replace('»', '')\n",
    "    doc = doc.replace('.', '. ')\n",
    "    doc = doc.replace(' .', '.')\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace(']', '')\n",
    "    doc = doc.replace('...', '')\n",
    "    doc = doc.replace(' !', '!')\n",
    "    doc = doc.replace(' ?', '?')\n",
    "    doc = doc.replace(' ,', ',')\n",
    "    doc = doc.replace('( ', '(')\n",
    "    doc = doc.replace(' )', ')')\n",
    "    tokens = doc.split()\n",
    "    tokens = [token.lower() for token in tokens if token != '']\n",
    "    print(\"Clean text completed\")\n",
    "    print(f'Total tokens: {len(tokens)}')\n",
    "    print(f'Unique tokens: {len(set(tokens))}')\n",
    "    return tokens\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "def to_sequences(length, tokens):\n",
    "    sequences = []\n",
    "    for i in range(length, len(tokens)):\n",
    "        seq = tokens[i-length:i]\n",
    "        if len(seq) == length:\n",
    "            line = ' '.join(seq)\n",
    "            sequences.append(line)\n",
    "        else:\n",
    "            sequences\n",
    "    print(\"Sequences formed\")\n",
    "    print(f'Total Sequences: {len(sequences)}')\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de una métrica \n",
    "Es importante tener en cuenta que en la generación de texto el `accuracy` no es buena medida para el rendimiento del modelo. Por ello, además de considerar el accuracy como métrica, consideraremos la perplejidad (perplexity). \n",
    "\n",
    "Teniendo en cuenta la entropía cruzada, la cual puede ser interpretada como el número medio de bits necesarios para almacenar la información de una variable, donde p es la distribución real de nuestro lenguaje y q es la distribución estimada de nuestro modelo. Dado que no se conoce el valor de p, dada una secuencia de palabras $\\omega$ de longitud $N$ y haciendo uso del teorema de Shannon-McMillan-Breiman, se puede aproximar la entropía cruzada por palabra como: \n",
    "\n",
    "$$H(p,q) \\approx  -\\dfrac{1}{N}*log_{2}q(\\omega)$$ \n",
    "\n",
    "Teniendo esto en cuenta, podemos definir la perplejidad como el exponencial de la entropía cruzada \n",
    "\n",
    "$$P(p,q) = e^{H(p,q)}$$\n",
    "\n",
    "Lo cual no indica el numero promedio de palabras que se pueden codificar usando $H(\\omega)$ bits, es decir, representa el número de palabras que son posibles en cada punto de la secuencia. Para dejar en claro esto, si se tiene una perplejidad de 50, quiere decir que cada vez que el modelo intenta adivinar la siguiente palabra, esta tan confundido como si tuviera que elegir entre 50 palabras. Podemos ver que, entre menor sea el número de perplejidad, mejor será el rendimiento del modelo. No obstante, una perplejidad de 1 nos indicaría un sobreajuste en los datos, sin embargo, este es un caso aislado. \n",
    "\n",
    "Ahora, definimos la siguiente función para calcular la perplejidad durante el entrenamiento de cada uno de los modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = tf.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos \n",
    "\n",
    "Se consideraron dos textos. El primero de ellos está compuesto por la saga de Harry Potter. El segundo es el libro El Código Da Vinci. Ambos textos cargados y se aplica el preprocesamiento a cada uno. Posteriormente, el texto limpio es guardado en un nuevo archivo de texto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus 1 \n",
    "\n",
    "Podemos observar que este corpus cuenta con 208.782 palabras, un vocabulario (Palabras sin repetir) de 26.661 y en total se forman 208.731 secuencias de 51 palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean text completed\n",
      "Total tokens: 208782\n",
      "Unique tokens: 26661\n",
      "Sequences formed\n",
      "Total Sequences: 208731\n"
     ]
    }
   ],
   "source": [
    "in_filenames = ['Books_data/HarryPotter/J.K. Rowling - Harry Potter 1 - La Piedra Filosofal.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 2 - La Cámara Secreta.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 3 - El Prisionero de Azkaban.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 4 - El Cáliz de Fuego.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 5 - La Orden del Fenix.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 6 - El Misterio del Príncipe.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 7 - Las Reliquias de la Muerte.txt']\n",
    "doc = load_docs(in_filenames)\n",
    "tokens = clean_text(doc)\n",
    "lines = to_sequences(51, tokens)\n",
    "\n",
    "out_filename = 'HarryPoter_CleanText.txt'\n",
    "save_doc(lines, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus 2 \n",
    "\n",
    "Podemos observar que este corpus cuenta con 153.890 palabras, un vocabulario (Palabras sin repetir) de 22.182 y en total se forman 153.839 secuencias de 51 palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean text completed\n",
      "Total tokens: 153890\n",
      "Unique tokens: 22182\n",
      "Sequences formed\n",
      "Total Sequences: 153839\n"
     ]
    }
   ],
   "source": [
    "in_filename = 'Books_data/El Codigo Da Vinci.txt'\n",
    "doc = load_doc(in_filename)\n",
    "tokens = clean_text(doc)\n",
    "lines = to_sequences(51, tokens)\n",
    "\n",
    "out_filename = 'CodigoDaVinci_CleanText.txt'\n",
    "save_doc(lines, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos para el corpus 1 (Harry Potter) \n",
    "Primero, se configura el uso de GPU para acelerar el tiempo de ejecución (entrenamiento) del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el archivo que contiene el texto ya procesado y se realiza la tokenizacion del mismo. También, se definen las secuencias de acuerdo con los tokens definidos (índices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "\n",
    "in_filename = 'HarryPoter_CleanText.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "sequences = [sequence for sequence in sequences if len(sequence) == 51]\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define el modelo el cual consta de una capa de embedding, dos capas de Long short-term memory (LSTM) y 2 capas densas, la primera con una función de activación relu y la segunda con una función de activación softmax. Como optimizador se utiliza Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            886050    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 80)            41920     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 80)                51520     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17721)             903771    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,887,311\n",
      "Trainable params: 1,887,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "seq_length = 50 #X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(80, return_sequences=True))\n",
    "model.add(LSTM(80))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado el tamaño de los datos, la arquitectura del modelo y la capacidad computacional disponible, los datos son divididos en 4 secciones y cada sección se presenta al modelo con un batch size de 128 y 40 épocas. Los resultados del entrenamiento son almacenados en los archivos `model_HarryPotter.h5` y `tokenizer_HarryPotter.pkl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 20:16:22.853824: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n",
      "2022-04-29 20:16:27.278876: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 10s 15ms/step - loss: 7.3191 - accuracy: 0.0438 - perplexity: 2186.3201\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.7323 - accuracy: 0.0474 - perplexity: 864.6395\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.5118 - accuracy: 0.0636 - perplexity: 692.0182\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.3132 - accuracy: 0.0694 - perplexity: 567.4196\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 6.1697 - accuracy: 0.0727 - perplexity: 489.3873\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 6.0364 - accuracy: 0.0797 - perplexity: 431.1900\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.2971 - accuracy: 0.1308 - perplexity: 204.5429\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.2049 - accuracy: 0.1361 - perplexity: 186.4925\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.1200 - accuracy: 0.1398 - perplexity: 171.4547\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.0358 - accuracy: 0.1443 - perplexity: 157.6333\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9594 - accuracy: 0.1477 - perplexity: 145.3454\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.8838 - accuracy: 0.1507 - perplexity: 135.0729\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.8092 - accuracy: 0.1546 - perplexity: 125.6122\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.7383 - accuracy: 0.1583 - perplexity: 116.9769\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.6637 - accuracy: 0.1626 - perplexity: 108.3002\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5939 - accuracy: 0.1649 - perplexity: 101.1726\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5231 - accuracy: 0.1701 - perplexity: 94.0277\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4541 - accuracy: 0.1734 - perplexity: 87.5816\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3880 - accuracy: 0.1756 - perplexity: 82.2599\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3220 - accuracy: 0.1814 - perplexity: 76.7797\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2606 - accuracy: 0.1847 - perplexity: 72.4241\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.5079 - accuracy: 0.1310 - perplexity: 254.3818\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.3992 - accuracy: 0.1385 - perplexity: 227.3705\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.2914 - accuracy: 0.1456 - perplexity: 204.7798\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.1906 - accuracy: 0.1528 - perplexity: 184.8183\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.0941 - accuracy: 0.1585 - perplexity: 168.4349\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9982 - accuracy: 0.1662 - perplexity: 152.0286\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9131 - accuracy: 0.1722 - perplexity: 140.3096\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5277 - accuracy: 0.2004 - perplexity: 95.0073\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4557 - accuracy: 0.2043 - perplexity: 88.0180\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.7417 - accuracy: 0.2879 - perplexity: 43.1257\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.7053 - accuracy: 0.2904 - perplexity: 41.6548\n"
     ]
    }
   ],
   "source": [
    "size_data = len(sequences)//4\n",
    "\n",
    "for i in range(size_data,len(sequences)+1, size_data):\n",
    "    sub_sequences = sequences[i-size_data:i]\n",
    "    sequences_aux = array(sub_sequences)\n",
    "    \n",
    "    X, y = sequences_aux[:,:-1], sequences_aux[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "    model.fit(X, y, batch_size=128, epochs=40)\n",
    "    \n",
    "    # save the model to file\n",
    "    model.save('model_HarryPotter.h5')\n",
    "    # save the tokenizer\n",
    "    dump(tokenizer, open('tokenizer_HarryPotter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 04:55:47.100893: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697167672 exceeds 10% of free system memory.\n",
      "2022-04-30 04:55:51.406761: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697167672 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 8s 16ms/step - loss: 4.0449 - accuracy: 0.2918 - perplexity: 59.1985\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 4.0104 - accuracy: 0.2939 - perplexity: 57.2866\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.9729 - accuracy: 0.3002 - perplexity: 55.1830\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.9408 - accuracy: 0.3020 - perplexity: 53.3407\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.9027 - accuracy: 0.3057 - perplexity: 51.2433\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.8689 - accuracy: 0.3110 - perplexity: 49.4850\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.8376 - accuracy: 0.3133 - perplexity: 48.1481\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.8090 - accuracy: 0.3154 - perplexity: 46.9789\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.7758 - accuracy: 0.3196 - perplexity: 45.1913\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.7415 - accuracy: 0.3225 - perplexity: 43.6553\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.7170 - accuracy: 0.3250 - perplexity: 42.5915\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.6859 - accuracy: 0.3292 - perplexity: 41.2744\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.6568 - accuracy: 0.3322 - perplexity: 39.8630\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.6276 - accuracy: 0.3343 - perplexity: 38.8770\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.5970 - accuracy: 0.3382 - perplexity: 37.8786\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.5715 - accuracy: 0.3405 - perplexity: 36.6866\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.5402 - accuracy: 0.3431 - perplexity: 35.8075\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.5132 - accuracy: 0.3477 - perplexity: 34.5695\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.4873 - accuracy: 0.3489 - perplexity: 33.8347\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.4571 - accuracy: 0.3531 - perplexity: 32.7370\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.4292 - accuracy: 0.3544 - perplexity: 31.7698\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.4028 - accuracy: 0.3600 - perplexity: 31.0780\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.3789 - accuracy: 0.3617 - perplexity: 30.3022\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.3523 - accuracy: 0.3651 - perplexity: 29.5058\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.3273 - accuracy: 0.3680 - perplexity: 28.7264\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.3020 - accuracy: 0.3697 - perplexity: 28.0766\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.2742 - accuracy: 0.3737 - perplexity: 27.1639\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.2479 - accuracy: 0.3770 - perplexity: 26.4254\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.2204 - accuracy: 0.3809 - perplexity: 25.7568\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.1977 - accuracy: 0.3806 - perplexity: 25.1518\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.1724 - accuracy: 0.3850 - perplexity: 24.5633\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.1463 - accuracy: 0.3892 - perplexity: 23.9588\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.1235 - accuracy: 0.3921 - perplexity: 23.4365\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.0996 - accuracy: 0.3939 - perplexity: 22.8866\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.0794 - accuracy: 0.3962 - perplexity: 22.3713\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 3.0616 - accuracy: 0.3982 - perplexity: 21.9391\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.0322 - accuracy: 0.4010 - perplexity: 21.2702\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.0086 - accuracy: 0.4052 - perplexity: 20.7995\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 2.9875 - accuracy: 0.4062 - perplexity: 20.4199\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 6s 16ms/step - loss: 2.9622 - accuracy: 0.4103 - perplexity: 19.7951\n"
     ]
    }
   ],
   "source": [
    "size_data = int(len(sequences)*(3/4))\n",
    "\n",
    "\n",
    "sub_sequences = sequences[size_data:]\n",
    "sequences_aux = array(sub_sequences)\n",
    "    \n",
    "X, y = sequences_aux[:,:-1], sequences_aux[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "model = load_model('model_HarryPotter.h5', custom_objects={'perplexity':perplexity}, compile=True)\n",
    "tokenizer = load(open('tokenizer_HarryPotter.pkl', 'rb'))\n",
    "\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=40)\n",
    "    \n",
    "# save the model to file\n",
    "model.save('model_HarryPotter.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer_HarryPotter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo modelo\n",
    "Al igual que el primer modelo, esta arquitectura consta de una capa de embedding, dos capas de LSTM y 2 capas densas con funciones de activación relu y softmax respectivamente. Para este modelo se consideró una menor cantidad de neuronas, disminuyendo los parámetros a entrenar en un 23% aproximadamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            886050    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 20)            5680      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 20)                3280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                630       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17721)             549351    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,444,991\n",
      "Trainable params: 1,444,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "seq_length = 50 #X.shape[1]\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model3.add(LSTM(20, return_sequences=True))\n",
    "model3.add(LSTM(20))\n",
    "model3.add(Dense(30, activation='relu'))\n",
    "model3.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model3.summary())\n",
    "# compile model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el modelo anterior, dadas las capacidades computaciones, los datos son divididos y presentados al modelo en 4 secciones con un batch size de 128 y 40 épocas. Los resultados del modelo son almacenados en los archivos `model2_HarryPotter.h5` y `tokenizer2_HarryPotter.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 21:18:53.879613: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n",
      "2022-04-29 21:18:58.306205: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 9s 13ms/step - loss: 7.4211 - accuracy: 0.0449 - perplexity: 2863.1711\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.7393 - accuracy: 0.0463 - perplexity: 866.3502\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.6363 - accuracy: 0.0521 - perplexity: 785.2633\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.5011 - accuracy: 0.0634 - perplexity: 684.5125\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.3538 - accuracy: 0.0661 - perplexity: 589.8724\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.2188 - accuracy: 0.0693 - perplexity: 515.4156\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.0890 - accuracy: 0.0771 - perplexity: 453.2469\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.9616 - accuracy: 0.0860 - perplexity: 400.4470\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.8516 - accuracy: 0.0909 - perplexity: 357.7480\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.7608 - accuracy: 0.0979 - perplexity: 325.7560\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.6810 - accuracy: 0.1046 - perplexity: 302.2546\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.6062 - accuracy: 0.1109 - perplexity: 278.6839\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.5323 - accuracy: 0.1161 - perplexity: 258.9629\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.4609 - accuracy: 0.1211 - perplexity: 242.4658\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.3883 - accuracy: 0.1260 - perplexity: 224.9827\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.3181 - accuracy: 0.1288 - perplexity: 209.6524\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.2479 - accuracy: 0.1317 - perplexity: 195.2152\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.1799 - accuracy: 0.1352 - perplexity: 182.1959\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.1131 - accuracy: 0.1376 - perplexity: 170.0367\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.0462 - accuracy: 0.1420 - perplexity: 159.0256\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.9793 - accuracy: 0.1453 - perplexity: 148.8509\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.9115 - accuracy: 0.1484 - perplexity: 138.8549\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.8456 - accuracy: 0.1533 - perplexity: 129.8038\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.7769 - accuracy: 0.1576 - perplexity: 121.5106\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.7096 - accuracy: 0.1598 - perplexity: 113.9369\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.6409 - accuracy: 0.1671 - perplexity: 106.0600\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5771 - accuracy: 0.1688 - perplexity: 99.5755\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5128 - accuracy: 0.1738 - perplexity: 93.2064\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.4529 - accuracy: 0.1777 - perplexity: 87.8798\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3926 - accuracy: 0.1806 - perplexity: 82.7383\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3348 - accuracy: 0.1861 - perplexity: 77.9833\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.2781 - accuracy: 0.1899 - perplexity: 73.5356\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.2257 - accuracy: 0.1933 - perplexity: 69.7706\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1696 - accuracy: 0.1987 - perplexity: 65.9924\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1225 - accuracy: 0.2018 - perplexity: 63.1399\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0715 - accuracy: 0.2059 - perplexity: 59.9393\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0227 - accuracy: 0.2100 - perplexity: 57.0065\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9788 - accuracy: 0.2151 - perplexity: 54.5259\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9353 - accuracy: 0.2202 - perplexity: 52.2007\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.8927 - accuracy: 0.2261 - perplexity: 50.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 21:22:41.868880: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n",
      "2022-04-29 21:22:46.287030: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 7.2949 - accuracy: 0.0656 - perplexity: 14176.2422\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.3057 - accuracy: 0.0820 - perplexity: 571.3538\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.1146 - accuracy: 0.0933 - perplexity: 468.9853\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.9541 - accuracy: 0.1053 - perplexity: 398.6401\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.8061 - accuracy: 0.1153 - perplexity: 342.9427\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.6672 - accuracy: 0.1245 - perplexity: 297.6137\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.5416 - accuracy: 0.1330 - perplexity: 262.2934\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.4206 - accuracy: 0.1402 - perplexity: 232.6329\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.3041 - accuracy: 0.1484 - perplexity: 207.6690\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.1950 - accuracy: 0.1549 - perplexity: 185.7794\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.0882 - accuracy: 0.1610 - perplexity: 166.8253\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.9900 - accuracy: 0.1683 - perplexity: 151.5927\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.8973 - accuracy: 0.1742 - perplexity: 138.1741\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.8066 - accuracy: 0.1785 - perplexity: 125.4774\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.7218 - accuracy: 0.1845 - perplexity: 115.4454\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.6376 - accuracy: 0.1897 - perplexity: 106.1123\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5556 - accuracy: 0.1949 - perplexity: 97.7283\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.4764 - accuracy: 0.1996 - perplexity: 90.1647\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3996 - accuracy: 0.2047 - perplexity: 83.4923\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3253 - accuracy: 0.2103 - perplexity: 77.5574\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.2522 - accuracy: 0.2143 - perplexity: 72.1123\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1832 - accuracy: 0.2208 - perplexity: 67.0759\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1103 - accuracy: 0.2258 - perplexity: 62.5234\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0457 - accuracy: 0.2303 - perplexity: 58.5962\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9795 - accuracy: 0.2357 - perplexity: 54.8699\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9152 - accuracy: 0.2402 - perplexity: 51.3639\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.8526 - accuracy: 0.2456 - perplexity: 48.1098\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 6s 13ms/step - loss: 3.7932 - accuracy: 0.2502 - perplexity: 45.2688\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.7326 - accuracy: 0.2574 - perplexity: 42.8223\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.6726 - accuracy: 0.2618 - perplexity: 40.3001\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.6147 - accuracy: 0.2684 - perplexity: 37.8600\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.5606 - accuracy: 0.2750 - perplexity: 35.9571\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.5050 - accuracy: 0.2809 - perplexity: 33.9127\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.4575 - accuracy: 0.2858 - perplexity: 32.4727\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.4024 - accuracy: 0.2914 - perplexity: 30.6670\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.3515 - accuracy: 0.3001 - perplexity: 29.1267\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.3052 - accuracy: 0.3049 - perplexity: 27.8563\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.2562 - accuracy: 0.3105 - perplexity: 26.4997\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.2132 - accuracy: 0.3194 - perplexity: 25.3083\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.1656 - accuracy: 0.3276 - perplexity: 24.1603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 21:26:26.760787: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 7.5595 - accuracy: 0.0612 - perplexity: 79606.2031\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.5005 - accuracy: 0.0838 - perplexity: 691.2253\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.2476 - accuracy: 0.0917 - perplexity: 532.5765\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.0664 - accuracy: 0.1002 - perplexity: 446.4307\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.9113 - accuracy: 0.1099 - perplexity: 383.6451\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.7713 - accuracy: 0.1191 - perplexity: 331.6772\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.6400 - accuracy: 0.1280 - perplexity: 292.1404\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.5151 - accuracy: 0.1357 - perplexity: 256.9824\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.3995 - accuracy: 0.1445 - perplexity: 229.0852\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.2883 - accuracy: 0.1505 - perplexity: 204.2643\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.1855 - accuracy: 0.1565 - perplexity: 183.8679\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.0864 - accuracy: 0.1633 - perplexity: 166.5400\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.9929 - accuracy: 0.1712 - perplexity: 152.1314\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.9016 - accuracy: 0.1769 - perplexity: 138.6345\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.8161 - accuracy: 0.1825 - perplexity: 127.4918\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.7341 - accuracy: 0.1893 - perplexity: 117.1965\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.6554 - accuracy: 0.1963 - perplexity: 108.1055\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5802 - accuracy: 0.2028 - perplexity: 100.3682\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5057 - accuracy: 0.2095 - perplexity: 93.2211\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.4362 - accuracy: 0.2145 - perplexity: 86.9449\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3670 - accuracy: 0.2214 - perplexity: 81.3063\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3002 - accuracy: 0.2273 - perplexity: 75.7041\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.2335 - accuracy: 0.2334 - perplexity: 71.0167\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1721 - accuracy: 0.2394 - perplexity: 66.7771\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1096 - accuracy: 0.2461 - perplexity: 62.8298\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0501 - accuracy: 0.2511 - perplexity: 59.1238\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9928 - accuracy: 0.2563 - perplexity: 55.6709\n",
      "Epoch 28/40\n",
      "121/408 [=======>......................] - ETA: 3s - loss: 3.8944 - accuracy: 0.2681 - perplexity: 50.4284"
     ]
    }
   ],
   "source": [
    "size_data = len(sequences)//4\n",
    "\n",
    "for i in range(size_data,len(sequences)+1, size_data):\n",
    "    sub_sequences = sequences[i-size_data:i]\n",
    "    sequences_aux = array(sub_sequences)\n",
    "    \n",
    "    X, y = sequences_aux[:,:-1], sequences_aux[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "    model3.fit(X, y, batch_size=128, epochs=40)\n",
    "    \n",
    "    # save the model to file\n",
    "    model3.save('model2_HarryPotter.h5')\n",
    "    # save the tokenizer\n",
    "    dump(tokenizer, open('tokenizer2_HarryPotter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 04:50:04.745587: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697167672 exceeds 10% of free system memory.\n",
      "2022-04-30 04:50:09.150621: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697167672 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 9s 14ms/step - loss: 7.6512 - accuracy: 0.0602 - perplexity: 9830.4658\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.7730 - accuracy: 0.0919 - perplexity: 931.9248\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.4982 - accuracy: 0.1040 - perplexity: 698.9741\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.3044 - accuracy: 0.1118 - perplexity: 578.8286\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.1460 - accuracy: 0.1194 - perplexity: 489.7347\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.0046 - accuracy: 0.1245 - perplexity: 425.1531\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.8736 - accuracy: 0.1324 - perplexity: 372.6039\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.7545 - accuracy: 0.1402 - perplexity: 331.1023\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.6396 - accuracy: 0.1471 - perplexity: 294.2284\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.5315 - accuracy: 0.1547 - perplexity: 264.0624\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.4303 - accuracy: 0.1616 - perplexity: 238.8344\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.3327 - accuracy: 0.1684 - perplexity: 216.7008\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.2403 - accuracy: 0.1753 - perplexity: 196.4202\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.1513 - accuracy: 0.1831 - perplexity: 180.8105\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.0685 - accuracy: 0.1889 - perplexity: 166.0452\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.9883 - accuracy: 0.1953 - perplexity: 152.6088\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.9118 - accuracy: 0.2017 - perplexity: 141.4943\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.8372 - accuracy: 0.2076 - perplexity: 131.8687\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.7678 - accuracy: 0.2147 - perplexity: 122.5260\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.7010 - accuracy: 0.2199 - perplexity: 114.8018\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.6386 - accuracy: 0.2272 - perplexity: 107.9802\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.5746 - accuracy: 0.2337 - perplexity: 101.5907\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.5133 - accuracy: 0.2389 - perplexity: 95.1826\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.4549 - accuracy: 0.2453 - perplexity: 90.0332\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.3994 - accuracy: 0.2500 - perplexity: 84.9109\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.3471 - accuracy: 0.2559 - perplexity: 80.4368\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.2916 - accuracy: 0.2605 - perplexity: 76.1212\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.2400 - accuracy: 0.2660 - perplexity: 71.9362\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.1883 - accuracy: 0.2701 - perplexity: 68.3828\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.1402 - accuracy: 0.2754 - perplexity: 65.4271\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.0939 - accuracy: 0.2806 - perplexity: 62.4622\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.0455 - accuracy: 0.2851 - perplexity: 59.3157\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.0011 - accuracy: 0.2903 - perplexity: 56.8103\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.9562 - accuracy: 0.2945 - perplexity: 54.2388\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.9131 - accuracy: 0.2984 - perplexity: 51.8090\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.8744 - accuracy: 0.3027 - perplexity: 49.7502\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.8301 - accuracy: 0.3062 - perplexity: 47.8956\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.7908 - accuracy: 0.3113 - perplexity: 45.9150\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.7504 - accuracy: 0.3155 - perplexity: 43.9763\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.7146 - accuracy: 0.3199 - perplexity: 42.2630\n"
     ]
    }
   ],
   "source": [
    "size_data = int(len(sequences)*(3/4))\n",
    "\n",
    "\n",
    "sub_sequences = sequences[size_data:]\n",
    "sequences_aux = array(sub_sequences)\n",
    "    \n",
    "X, y = sequences_aux[:,:-1], sequences_aux[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "model = load_model('model2_HarryPotter.h5', custom_objects={'perplexity':perplexity}, compile=True)\n",
    "tokenizer = load(open('tokenizer2_HarryPotter.pkl', 'rb'))\n",
    "\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=40)\n",
    "    \n",
    "# save the model to file\n",
    "model.save('model2_HarryPotter.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer2_HarryPotter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelos para el corpus 2 (Código Da Vinci) \n",
    "Configuramos el uso de la GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo que contiene los datos limpios, se realiza la tokenizacion y se crean las secuencias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "in_filename = 'CodigoDaVinci_CleanText.txt'\n",
    "doc2 = load_doc(in_filename)\n",
    "lines2 = doc2.split('\\n')\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(lines2)\n",
    "sequences2 = tokenizer2.texts_to_sequences(lines2)\n",
    "sequences2 = [sequence for sequence in sequences2 if len(sequence) == 51]\n",
    "vocab_size2 = len(tokenizer2.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer modelo \n",
    "Se define el modelo. Observe que al primr modelo del copus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            774550    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 80)            41920     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 80)                51520     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15491)             790041    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,662,081\n",
      "Trainable params: 1,662,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "seq_length2 = 50 #X.shape[1]\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size2, 50, input_length=seq_length2))\n",
    "model2.add(LSTM(80, return_sequences=True))\n",
    "model2.add(LSTM(80))\n",
    "model2.add(Dense(50, activation='relu'))\n",
    "model2.add(Dense(vocab_size2, activation='softmax'))\n",
    "print(model2.summary())\n",
    "# compile model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de este corpus, dado que es más pequeño que el primero (cuenta con tres cuartas partes de la cantidad de palabras), los datos se dividieron en 3 secciones y, de igual manera, se entrenó con un batch size de 128 con la diferencia de que el entrenamiento se realiza por 50 épocas. Los resultados del modelo se almacenan en los archivos `model_DaVinci.h5` y `tokenizer_DaVinci.pkl` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 01:22:46.152647: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n",
      "2022-04-29 01:22:49.461103: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 9s 15ms/step - loss: 7.2695 - accuracy: 0.0527 - perplexity: 2087.9705\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.6903 - accuracy: 0.0555 - perplexity: 831.7921\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.4557 - accuracy: 0.0742 - perplexity: 652.9443\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.2426 - accuracy: 0.0795 - perplexity: 529.7634\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.1127 - accuracy: 0.0836 - perplexity: 464.4283\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.9833 - accuracy: 0.0908 - perplexity: 407.9745\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.8426 - accuracy: 0.1020 - perplexity: 355.9132\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.7216 - accuracy: 0.1112 - perplexity: 313.1197\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.6135 - accuracy: 0.1181 - perplexity: 281.5971\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.5176 - accuracy: 0.1208 - perplexity: 255.6802\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.4318 - accuracy: 0.1268 - perplexity: 234.7844\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.3535 - accuracy: 0.1285 - perplexity: 217.3880\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.2801 - accuracy: 0.1347 - perplexity: 201.3783\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.2100 - accuracy: 0.1380 - perplexity: 187.1681\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.1400 - accuracy: 0.1428 - perplexity: 175.4236\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.0728 - accuracy: 0.1460 - perplexity: 163.4472\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.0087 - accuracy: 0.1491 - perplexity: 153.1077\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.9417 - accuracy: 0.1515 - perplexity: 143.2818\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.8788 - accuracy: 0.1561 - perplexity: 134.4336\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.8151 - accuracy: 0.1590 - perplexity: 126.2854\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.7523 - accuracy: 0.1621 - perplexity: 118.2528\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.6921 - accuracy: 0.1658 - perplexity: 111.3305\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.6290 - accuracy: 0.1709 - perplexity: 104.7867\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.5730 - accuracy: 0.1726 - perplexity: 99.0860\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.5153 - accuracy: 0.1778 - perplexity: 93.4986\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.4581 - accuracy: 0.1799 - perplexity: 88.1164\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.4018 - accuracy: 0.1858 - perplexity: 83.3028\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.3487 - accuracy: 0.1882 - perplexity: 78.8632\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.2972 - accuracy: 0.1909 - perplexity: 75.1959\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.2443 - accuracy: 0.1929 - perplexity: 71.1162\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.1961 - accuracy: 0.1990 - perplexity: 67.7066\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.1469 - accuracy: 0.2027 - perplexity: 64.5478\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.0998 - accuracy: 0.2051 - perplexity: 61.6263\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.0538 - accuracy: 0.2099 - perplexity: 58.8720\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.0086 - accuracy: 0.2139 - perplexity: 56.3365\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.9671 - accuracy: 0.2181 - perplexity: 53.9332\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.9225 - accuracy: 0.2217 - perplexity: 51.5356\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.8847 - accuracy: 0.2245 - perplexity: 49.6504\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.8458 - accuracy: 0.2290 - perplexity: 47.9606\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8054 - accuracy: 0.2314 - perplexity: 46.0233\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.7642 - accuracy: 0.2363 - perplexity: 43.9350\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.7278 - accuracy: 0.2405 - perplexity: 42.4115\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.6945 - accuracy: 0.2454 - perplexity: 41.0792\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.6584 - accuracy: 0.2484 - perplexity: 39.6860\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.6236 - accuracy: 0.2519 - perplexity: 38.3015\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.5924 - accuracy: 0.2541 - perplexity: 37.0962\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.5592 - accuracy: 0.2601 - perplexity: 35.8738\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.5252 - accuracy: 0.2643 - perplexity: 34.6072\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.4954 - accuracy: 0.2690 - perplexity: 33.6574\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.4618 - accuracy: 0.2728 - perplexity: 32.4982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 01:27:05.510888: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n",
      "2022-04-29 01:27:08.796385: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 7.7337 - accuracy: 0.0557 - perplexity: 59893.7969\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.5630 - accuracy: 0.0827 - perplexity: 735.3923\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.3054 - accuracy: 0.0905 - perplexity: 564.2772\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.1314 - accuracy: 0.0970 - perplexity: 474.3522\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.9900 - accuracy: 0.1039 - perplexity: 413.1138\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.8592 - accuracy: 0.1108 - perplexity: 359.6851\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.7423 - accuracy: 0.1192 - perplexity: 320.8093\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.6312 - accuracy: 0.1287 - perplexity: 289.1989\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.5265 - accuracy: 0.1361 - perplexity: 259.9535\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.4300 - accuracy: 0.1436 - perplexity: 234.9733\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.3389 - accuracy: 0.1502 - perplexity: 214.6948\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.2509 - accuracy: 0.1562 - perplexity: 197.1640\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.1659 - accuracy: 0.1622 - perplexity: 180.3764\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.0843 - accuracy: 0.1692 - perplexity: 166.6928\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.0059 - accuracy: 0.1736 - perplexity: 153.8547\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.9292 - accuracy: 0.1810 - perplexity: 142.2275\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.8672 - accuracy: 0.1875 - perplexity: 134.1049\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.7899 - accuracy: 0.1922 - perplexity: 123.7973\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.7222 - accuracy: 0.1979 - perplexity: 115.7386\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.6555 - accuracy: 0.2029 - perplexity: 108.4212\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.5902 - accuracy: 0.2083 - perplexity: 101.4833\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.5286 - accuracy: 0.2138 - perplexity: 95.1145\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.4662 - accuracy: 0.2178 - perplexity: 89.6722\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.4068 - accuracy: 0.2237 - perplexity: 84.2161\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.3463 - accuracy: 0.2272 - perplexity: 79.1670\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.2858 - accuracy: 0.2322 - perplexity: 74.3052\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.2308 - accuracy: 0.2356 - perplexity: 70.5389\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.1732 - accuracy: 0.2395 - perplexity: 66.7231\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.1166 - accuracy: 0.2446 - perplexity: 63.0201\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.0607 - accuracy: 0.2480 - perplexity: 59.5474\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.0041 - accuracy: 0.2541 - perplexity: 56.2715\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.9526 - accuracy: 0.2571 - perplexity: 53.6414\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.9035 - accuracy: 0.2614 - perplexity: 50.8388\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.8463 - accuracy: 0.2650 - perplexity: 48.0317\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.7975 - accuracy: 0.2719 - perplexity: 45.8583\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.7472 - accuracy: 0.2753 - perplexity: 43.4743\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.7001 - accuracy: 0.2803 - perplexity: 41.5496\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.6527 - accuracy: 0.2832 - perplexity: 39.5333\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.6027 - accuracy: 0.2898 - perplexity: 37.6275\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.5564 - accuracy: 0.2942 - perplexity: 35.8836\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.0878 - accuracy: 0.2522 - perplexity: 88.9789\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.0397 - accuracy: 0.2481 - perplexity: 58.1371\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.8759 - accuracy: 0.2633 - perplexity: 49.4042\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.7666 - accuracy: 0.2710 - perplexity: 44.1216\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.6827 - accuracy: 0.2791 - perplexity: 40.6837\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.6182 - accuracy: 0.2855 - perplexity: 38.1437\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.5515 - accuracy: 0.2915 - perplexity: 35.5836\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.5161 - accuracy: 0.2948 - perplexity: 34.3824\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.4583 - accuracy: 0.3017 - perplexity: 32.4930\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.3971 - accuracy: 0.3084 - perplexity: 30.4976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 01:31:21.126578: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 7.1849 - accuracy: 0.0542 - perplexity: 70771.8516\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.4129 - accuracy: 0.0708 - perplexity: 632.7710\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.2338 - accuracy: 0.0809 - perplexity: 531.6660\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 6.1066 - accuracy: 0.0866 - perplexity: 466.0289\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.9986 - accuracy: 0.0908 - perplexity: 415.7966\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.8901 - accuracy: 0.0977 - perplexity: 370.9175\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.7902 - accuracy: 0.1048 - perplexity: 337.8956\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.6966 - accuracy: 0.1109 - perplexity: 307.5073\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.6029 - accuracy: 0.1182 - perplexity: 279.6288\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.5107 - accuracy: 0.1266 - perplexity: 255.7107\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.4122 - accuracy: 0.1345 - perplexity: 231.4931\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.3363 - accuracy: 0.1417 - perplexity: 214.4426\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.2506 - accuracy: 0.1460 - perplexity: 196.5569\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.1504 - accuracy: 0.1541 - perplexity: 177.9412\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 5.0525 - accuracy: 0.1614 - perplexity: 161.2678\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.9592 - accuracy: 0.1697 - perplexity: 148.1742\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.8775 - accuracy: 0.1765 - perplexity: 135.3055\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.8381 - accuracy: 0.1828 - perplexity: 131.0144\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.8349 - accuracy: 0.1865 - perplexity: 130.1626\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.7266 - accuracy: 0.1947 - perplexity: 117.1194\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.6380 - accuracy: 0.2020 - perplexity: 106.7039\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.5697 - accuracy: 0.2091 - perplexity: 99.3579\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.5000 - accuracy: 0.2149 - perplexity: 93.0743\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.3991 - accuracy: 0.2246 - perplexity: 84.1053\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.3235 - accuracy: 0.2330 - perplexity: 77.9301\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.2502 - accuracy: 0.2400 - perplexity: 72.2618\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.4035 - accuracy: 0.2352 - perplexity: 85.0390\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.2500 - accuracy: 0.2441 - perplexity: 72.7645\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.1126 - accuracy: 0.2549 - perplexity: 63.1677\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.1196 - accuracy: 0.2569 - perplexity: 64.0658\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 4.0008 - accuracy: 0.2696 - perplexity: 56.4526\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.9202 - accuracy: 0.2792 - perplexity: 51.8722\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.8504 - accuracy: 0.2882 - perplexity: 48.3516\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.7818 - accuracy: 0.2980 - perplexity: 45.2910\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.8209 - accuracy: 0.2969 - perplexity: 47.3626\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.7392 - accuracy: 0.3070 - perplexity: 43.4068\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.6562 - accuracy: 0.3167 - perplexity: 39.8418\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.5889 - accuracy: 0.3260 - perplexity: 37.1619\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.5214 - accuracy: 0.3345 - perplexity: 34.9038\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.4695 - accuracy: 0.3441 - perplexity: 33.1314\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.4245 - accuracy: 0.3499 - perplexity: 31.6431\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.3569 - accuracy: 0.3587 - perplexity: 29.6725\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.3108 - accuracy: 0.3662 - perplexity: 28.1555\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.2705 - accuracy: 0.3728 - perplexity: 27.2131\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.2455 - accuracy: 0.3775 - perplexity: 26.5046\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.1956 - accuracy: 0.3838 - perplexity: 25.1617\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.1832 - accuracy: 0.3878 - perplexity: 25.0339\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.2107 - accuracy: 0.3875 - perplexity: 25.9197\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.3812 - accuracy: 0.3724 - perplexity: 30.6286\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 5s 15ms/step - loss: 3.1582 - accuracy: 0.3943 - perplexity: 24.4215\n"
     ]
    }
   ],
   "source": [
    "size_data = len(sequences2)//3\n",
    "\n",
    "for i in range(size_data,len(sequences2)+1, size_data):\n",
    "    sub_sequences2 = sequences2[i-size_data:i]\n",
    "    sequences_aux2 = array(sub_sequences2)\n",
    "    \n",
    "    X, y = sequences_aux2[:,:-1], sequences_aux2[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size2)\n",
    "\n",
    "    model2.fit(X, y, batch_size=128, epochs=50)\n",
    "    \n",
    "# save the model to file\n",
    "model2.save('model_DaVinci.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer2, open('tokenizer_DaVinci.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo modelo\n",
    "Para este modelo, se tomó como base la arquitectura anterior, se añadió una capa LSTM con 50 neuronas y se disminuyó el número de neuronas en cada una de las capas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 50)            774550    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50, 80)            41920     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50, 65)            37960     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 50)                23200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 15491)             790041    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,670,221\n",
      "Trainable params: 1,670,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "seq_length2 = 50 #X.shape[1]\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(vocab_size2, 50, input_length=seq_length2))\n",
    "model4.add(LSTM(80, return_sequences=True))\n",
    "model4.add(LSTM(65, return_sequences=True))\n",
    "model4.add(LSTM(50))\n",
    "model4.add(Dense(50, activation='relu'))\n",
    "model4.add(Dense(vocab_size2, activation='softmax'))\n",
    "print(model4.summary())\n",
    "# compile model\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos son presentados al modelo en 3 secciones y se entrena con un batch size de 128 por 50 épocas. Los resultados del modelo son almacenados en los archivos `model2_DaVinci.h5` y `tokenizer2_DaVinci.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 10s 18ms/step - loss: 7.2871 - accuracy: 0.0537 - perplexity: 2203.3335\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.7697 - accuracy: 0.0541 - perplexity: 897.4904\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.7442 - accuracy: 0.0539 - perplexity: 872.8828\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.7349 - accuracy: 0.0541 - perplexity: 862.5467\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.7037 - accuracy: 0.0540 - perplexity: 837.5724\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.6098 - accuracy: 0.0579 - perplexity: 764.6805\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.4026 - accuracy: 0.0752 - perplexity: 620.6011\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.2160 - accuracy: 0.0808 - perplexity: 515.8389\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.0887 - accuracy: 0.0841 - perplexity: 453.5676\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.9943 - accuracy: 0.0867 - perplexity: 413.2537\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.9120 - accuracy: 0.0885 - perplexity: 379.3043\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.8169 - accuracy: 0.0917 - perplexity: 345.6649\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.7274 - accuracy: 0.0940 - perplexity: 316.0833\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.6452 - accuracy: 0.0973 - perplexity: 290.3120\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.5634 - accuracy: 0.1020 - perplexity: 268.0517\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.4812 - accuracy: 0.1062 - perplexity: 245.9339\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.3981 - accuracy: 0.1137 - perplexity: 226.2443\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.3185 - accuracy: 0.1201 - perplexity: 209.3095\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.2414 - accuracy: 0.1264 - perplexity: 193.5930\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.1666 - accuracy: 0.1315 - perplexity: 180.0935\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.0926 - accuracy: 0.1362 - perplexity: 166.4976\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.0220 - accuracy: 0.1405 - perplexity: 155.0331\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.9490 - accuracy: 0.1440 - perplexity: 144.5339\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.8784 - accuracy: 0.1493 - perplexity: 134.6114\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.8050 - accuracy: 0.1535 - perplexity: 124.7856\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7894 - accuracy: 0.1553 - perplexity: 127.8071\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.4832 - accuracy: 0.1051 - perplexity: 263.9357\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.9430 - accuracy: 0.1347 - perplexity: 143.0057\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7968 - accuracy: 0.1471 - perplexity: 123.4593\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7051 - accuracy: 0.1553 - perplexity: 113.0254\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6338 - accuracy: 0.1578 - perplexity: 104.6154\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.5662 - accuracy: 0.1631 - perplexity: 98.1177\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.4957 - accuracy: 0.1679 - perplexity: 91.7426\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.4308 - accuracy: 0.1719 - perplexity: 85.9673\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.3642 - accuracy: 0.1756 - perplexity: 80.1829\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2963 - accuracy: 0.1802 - perplexity: 75.0023\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2329 - accuracy: 0.1870 - perplexity: 70.1779\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.1679 - accuracy: 0.1899 - perplexity: 65.7738\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.1127 - accuracy: 0.1929 - perplexity: 62.3888\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0456 - accuracy: 0.2006 - perplexity: 58.2761\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9791 - accuracy: 0.2071 - perplexity: 54.5073\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9240 - accuracy: 0.2109 - perplexity: 51.4776\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8619 - accuracy: 0.2168 - perplexity: 48.4897\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8038 - accuracy: 0.2234 - perplexity: 45.7186\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7516 - accuracy: 0.2285 - perplexity: 43.4344\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6916 - accuracy: 0.2333 - perplexity: 40.8750\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6328 - accuracy: 0.2422 - perplexity: 38.5064\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.5808 - accuracy: 0.2488 - perplexity: 36.5405\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.5295 - accuracy: 0.2543 - perplexity: 34.7457\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.4829 - accuracy: 0.2614 - perplexity: 33.1405\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 6s 18ms/step - loss: 7.1022 - accuracy: 0.0759 - perplexity: 68930.3047\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.1686 - accuracy: 0.0901 - perplexity: 493.9407\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 6s 18ms/step - loss: 5.9556 - accuracy: 0.0984 - perplexity: 396.7521\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.8000 - accuracy: 0.1086 - perplexity: 339.5813\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.6596 - accuracy: 0.1183 - perplexity: 294.8714\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.5309 - accuracy: 0.1280 - perplexity: 258.8660\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 6s 18ms/step - loss: 5.4092 - accuracy: 0.1366 - perplexity: 229.1163\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.2931 - accuracy: 0.1434 - perplexity: 204.8570\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.1816 - accuracy: 0.1515 - perplexity: 183.5448\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.0785 - accuracy: 0.1607 - perplexity: 165.2053\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.9767 - accuracy: 0.1684 - perplexity: 148.6740\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.8793 - accuracy: 0.1746 - perplexity: 135.4394\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7845 - accuracy: 0.1823 - perplexity: 122.8073\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6954 - accuracy: 0.1893 - perplexity: 112.8566\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6095 - accuracy: 0.1950 - perplexity: 103.4774\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.5170 - accuracy: 0.2021 - perplexity: 93.9259\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.4370 - accuracy: 0.2072 - perplexity: 86.8737\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.3565 - accuracy: 0.2148 - perplexity: 79.7693\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2776 - accuracy: 0.2206 - perplexity: 74.0668\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2006 - accuracy: 0.2265 - perplexity: 68.2054\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.1291 - accuracy: 0.2334 - perplexity: 63.6072\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0509 - accuracy: 0.2410 - perplexity: 58.7010\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9721 - accuracy: 0.2465 - perplexity: 54.2347\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8987 - accuracy: 0.2526 - perplexity: 50.5441\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8348 - accuracy: 0.2588 - perplexity: 47.3219\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7623 - accuracy: 0.2641 - perplexity: 44.0766\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6985 - accuracy: 0.2719 - perplexity: 41.2893\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6241 - accuracy: 0.2775 - perplexity: 38.4112\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.5607 - accuracy: 0.2842 - perplexity: 35.9651\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.4922 - accuracy: 0.2902 - perplexity: 33.5679\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.4302 - accuracy: 0.2949 - perplexity: 31.5289\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.3700 - accuracy: 0.3035 - perplexity: 29.6334\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.3100 - accuracy: 0.3107 - perplexity: 27.9371\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.2471 - accuracy: 0.3187 - perplexity: 26.2782\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.1924 - accuracy: 0.3241 - perplexity: 24.8807\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.1340 - accuracy: 0.3312 - perplexity: 23.4448\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.0825 - accuracy: 0.3390 - perplexity: 22.2464\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.0271 - accuracy: 0.3458 - perplexity: 21.0670\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.9726 - accuracy: 0.3538 - perplexity: 19.9606\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.9244 - accuracy: 0.3627 - perplexity: 18.9629\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.8776 - accuracy: 0.3689 - perplexity: 18.1024\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.8339 - accuracy: 0.3764 - perplexity: 17.3209\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.7833 - accuracy: 0.3846 - perplexity: 16.5097\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.7416 - accuracy: 0.3896 - perplexity: 15.8432\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.6982 - accuracy: 0.3995 - perplexity: 15.1355\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.6563 - accuracy: 0.4049 - perplexity: 14.5267\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.6246 - accuracy: 0.4101 - perplexity: 14.0598\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5889 - accuracy: 0.4168 - perplexity: 13.5778\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5448 - accuracy: 0.4264 - perplexity: 12.9720\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5003 - accuracy: 0.4337 - perplexity: 12.4384\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 6s 18ms/step - loss: 7.4175 - accuracy: 0.0481 - perplexity: 121316.5703\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.4740 - accuracy: 0.0743 - perplexity: 672.3108\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.2559 - accuracy: 0.0827 - perplexity: 538.1702\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.1061 - accuracy: 0.0921 - perplexity: 461.7701\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.9742 - accuracy: 0.0987 - perplexity: 404.9054\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.8494 - accuracy: 0.1064 - perplexity: 357.7068\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.7273 - accuracy: 0.1129 - perplexity: 318.0112\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.6050 - accuracy: 0.1216 - perplexity: 280.5041\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.4813 - accuracy: 0.1300 - perplexity: 247.7025\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.3631 - accuracy: 0.1379 - perplexity: 220.8085\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.2441 - accuracy: 0.1464 - perplexity: 194.4916\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.1294 - accuracy: 0.1544 - perplexity: 174.2258\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.0188 - accuracy: 0.1625 - perplexity: 155.8405\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.9108 - accuracy: 0.1707 - perplexity: 139.4669\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.8051 - accuracy: 0.1777 - perplexity: 126.0343\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7058 - accuracy: 0.1857 - perplexity: 113.9049\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6034 - accuracy: 0.1955 - perplexity: 102.7864\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.5107 - accuracy: 0.2033 - perplexity: 93.5024\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.4212 - accuracy: 0.2124 - perplexity: 85.6744\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.3291 - accuracy: 0.2209 - perplexity: 77.9662\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2503 - accuracy: 0.2283 - perplexity: 72.3515\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.1695 - accuracy: 0.2359 - perplexity: 66.6243\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0948 - accuracy: 0.2432 - perplexity: 61.6204\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0118 - accuracy: 0.2516 - perplexity: 56.7760\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9361 - accuracy: 0.2599 - perplexity: 52.5635\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8610 - accuracy: 0.2680 - perplexity: 48.6610\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7967 - accuracy: 0.2750 - perplexity: 45.5637\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7275 - accuracy: 0.2835 - perplexity: 42.7852\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6550 - accuracy: 0.2894 - perplexity: 39.7374\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.5925 - accuracy: 0.2980 - perplexity: 37.2020\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.5267 - accuracy: 0.3025 - perplexity: 34.7679\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 6s 18ms/step - loss: 3.4604 - accuracy: 0.3111 - perplexity: 32.6098\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.3925 - accuracy: 0.3182 - perplexity: 30.4802\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.3374 - accuracy: 0.3247 - perplexity: 28.7483\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.2823 - accuracy: 0.3308 - perplexity: 27.3487\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.2198 - accuracy: 0.3370 - perplexity: 25.5956\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 6s 18ms/step - loss: 3.1617 - accuracy: 0.3466 - perplexity: 24.1857\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.1008 - accuracy: 0.3521 - perplexity: 22.7729\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.0519 - accuracy: 0.3600 - perplexity: 21.6781\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.0015 - accuracy: 0.3661 - perplexity: 20.5166\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.9427 - accuracy: 0.3740 - perplexity: 19.3461\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.8836 - accuracy: 0.3832 - perplexity: 18.2987\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.8392 - accuracy: 0.3879 - perplexity: 17.5170\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.7851 - accuracy: 0.3970 - perplexity: 16.5189\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.7314 - accuracy: 0.4072 - perplexity: 15.6818\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.6949 - accuracy: 0.4111 - perplexity: 15.0939\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.6469 - accuracy: 0.4200 - perplexity: 14.3926\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.6050 - accuracy: 0.4265 - perplexity: 13.7911\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5540 - accuracy: 0.4337 - perplexity: 13.1437\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5206 - accuracy: 0.4410 - perplexity: 12.6833\n"
     ]
    }
   ],
   "source": [
    "size_data = len(sequences2)//3\n",
    "\n",
    "for i in range(size_data,len(sequences2)+1, size_data):\n",
    "    sub_sequences2 = sequences2[i-size_data:i]\n",
    "    sequences_aux2 = array(sub_sequences2)\n",
    "    \n",
    "    X, y = sequences_aux2[:,:-1], sequences_aux2[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size2)\n",
    "\n",
    "    model4.fit(X, y, batch_size=128, epochs=50)\n",
    "    \n",
    "# save the model to file\n",
    "model4.save('model2_DaVinci.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer2, open('tokenizer2_DaVinci.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de texto\n",
    "Importamos las librerías necesarias para la generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        predict_yhat = model.predict(encoded, verbose=0)\n",
    "        yhat = argmax(predict_yhat, axis=1)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 1 - Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'HarryPoter_CleanText.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model_HarryPotter.h5', custom_objects={'perplexity':perplexity})\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer_HarryPotter.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para que su luz llegara hasta el fondo. vio un destello rojo intenso, una espada con relumbrantes rubíes en la empuñadura. la espada de gryffindor yacía en el fondo del agua. casi sin respirar, el muchacho se quedó mirándola fijamente. ¿cómo era posible? ¿cómo podía haber acabado en el fondo de\n",
      "\n",
      "los jardines por la diadema se había llegado y se quedó mcgonagall se había llegado y se había llegado y se quedó mcgonagall se había llegado y se quedó mcgonagall se había llegado y se había llegado y se había llegado y se quedó mcgonagall se había llegado y se\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corpus 1 - Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'HarryPoter_CleanText.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines2 = doc.split('\\n')\n",
    "seq_length2 = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model2 = load_model('model2_HarryPotter.h5', custom_objects={'perplexity':perplexity})\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer2 = load(open('tokenizer2_HarryPotter.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parte porque olía a gato: era la misma en que habían dormido la noche de la copa del mundo de quidditch. ¿el dueño de esta tienda no era un tal perkins del ministerio? preguntó mientras liberaba las piquetas. sí, pero por lo visto ya no la quería, porque tiene lumbago explicó\n",
      "\n",
      "a harry y la profesora mcgonagall se había quedado para morir y no había muerto se había quedado en el castillo y no había muerto se había quedado bien sino que se había quedado prohibido de slytherin lo serpiente que se había quedado y a su lado y harry se\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text2 = lines2[randint(0,len(lines2))]\n",
    "print(seed_text2 + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model2, tokenizer2, seq_length2, seed_text2, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 2 - Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'CodigoDaVinci_CleanText.txt'\n",
    "doc3 = load_doc(in_filename)\n",
    "lines3 = doc3.split('\\n')\n",
    "seq_length3 = len(lines3[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model3 = load_model('model_DaVinci.h5', custom_objects={'perplexity':perplexity})\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer3 = load(open('tokenizer_DaVinci.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para evitar que los demás le robaran las ideas. el caso era que el artista hacía siempre lo que le venía en gana. * * * en su fuero interno, sophie se alegró al ver que robert había captado lo que había querido decir. las primeras palabras puedo leerlas más o\n",
      "\n",
      "charles de orbes principia mahematica a pesar de allí langdon se había abierto en la tumba de la abadía de westminster gettum se había convertido en consecuencia en dirección de la espalda de kent británico cuerpos de pie a la iglesia del temple y el poema está en el chasquido\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text3 = lines3[randint(0,len(lines3))]\n",
    "print(seed_text3 + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated3 = generate_seq(model3, tokenizer3, seq_length3, seed_text3, 50)\n",
    "print(generated3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el primer corpus, podemos ver que, el primer modelo termino con una perplejidad de 19, mientras que el segundo obtuvo un valor de 42.  \n",
    "\n",
    "Analizando el texto generado con cada uno de los modelos, podemos ver que, para este caso, aunque se tiene más coherencia, el valor de perplejidad hace que sea más probable la repetición de una secuencia. En el segundo modelo, la secuencia, dado su alto valor de perplejidad presenta menos coherencia  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 2 - Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'CodigoDaVinci_CleanText.txt'\n",
    "doc4 = load_doc(in_filename)\n",
    "lines4 = doc4.split('\\n')\n",
    "seq_length4 = len(lines4[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model4 = load_model('model2_DaVinci.h5', custom_objects={'perplexity':perplexity})\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer4 = load(open('tokenizer2_DaVinci.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su identidad. ¿está seguro de que quiere que sea silas quien haga el trabajo? le había preguntado hacía menos de media hora, cuando le había ordenado que robaran la clave. puedo hacerlo yo mismo. pero el maestro había sido muy claro. silas nos ha servido sin problemas con los cuatro miembros\n",
      "\n",
      "del priorato de sir leigh que encontramos los apetece a los demás espada en la emoción que dudó sabía encontrar la iglesia que había dicho que incluía favor que sus muros de voz tanteó sophie de capaces siguiendo su familia se puede decir el orbe que es un reverendo s\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text4 = lines4[randint(0,len(lines4))]\n",
    "print(seed_text4 + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated4 = generate_seq(model4, tokenizer4, seq_length4, seed_text4, 50)\n",
    "print(generated4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar, luego del entrenamiento de cada modelo para este corpus que, el primer modelo termino con una perplejidad de 24 aproximadamente, mientras que el segundo con un valor de 12. Razón por la cual, se espera que el segundo modelo tenga un mejor resultado en la generación de texto. \n",
    "\n",
    "Podemos ver que, a pesar de la falta de conexidad entre la secuencia, el segundo modelo otorga un mejor resultado al momento de evaluar el tema sobre el cual se habla "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camilo Andres Gomez Vargas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
